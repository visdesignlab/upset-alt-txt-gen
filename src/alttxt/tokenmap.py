from typing import Any, Callable, Tuple, Union, Optional
from alttxt.models import DataModel, GrammarModel, Subset
from alttxt.enums import SubsetField, IndividualSetSize, IntersectionTrend
from alttxt.regionclass import *
import math
from collections import Counter
import numpy as np
from scipy import stats
from scipy.optimize import curve_fit


class TokenMap:
    """
    This class maps tokens from the grammar to strings.
    Each string is either defined directly in the mapping,
    or is the result of a function call. Functions called
    should be defined in this class, and should return a string.
    Result strings can contain tokens, which are represented
    surrounded by curly braces, e.g. {{token}}.
    While this class returns a string for each token fed to it,
    it is not responsible for the actual substitution of tokens
    or the overall generation of the text description.
    """
    TRUNCATION_LENGTH = 19  # Global constant for truncation length

    def __init__(
        self, data: DataModel, grammar: GrammarModel, title: Optional[str] = None
    ) -> None:
        """
        Initialize the Grammar class. Note that internal values
        are not recomputed if the data or grammar are changed.
        Params:
            data: Imported from a data file generated by Upset
            grammar: Imported from a grammar file generated by Upset
            title: The title of the plot, if any
        """
        self.data: DataModel = data
        self.grammar: GrammarModel = grammar
        self.title: Optional[str] = title

        # This defines the mapping of tokens to strings/functions
        # As with the rest of this class, the curly braces surrounding
        # tokens are left out.
        # Generally, tokens which are easy to compute are done in the map,
        # whereas more complex tokens are done in functions.
        # Since functions are only executed on run, they can be used to
        # optimize by moving expensive tokens into fuctions.
        self.map: dict[str, Union[str, float, int, Callable[[], str]]] = {
            # Title of the plot as a phrase (with verb), with null check
            "title": f"is titled: {self.title}" if self.title else "has no title",
            # Dataset description as attribute name
            "dataset_description": (
                f"The dataset shows attributes of {self.grammar.metaData.description.lower()}. "
                if self.grammar.metaData.description
                else ""
            ),
            # Set description as set name
            "set_description": f"{self.grammar.metaData.items.lower()}" if self.grammar.metaData.items else "elements",
            # largest by what factor
            "largest_factor": f" {self.truncate_separately(self.sort_subsets_by_key(SubsetField.SIZE, True)[0].name)} is the largest by a factor of {self.calculate_largest_factor()}." if self.calculate_largest_factor() >= 2 else "",
            # set intersection categorization text based on intersection type and size
            "empty_set_presence": f" The empty intersection is present with a size of {self.get_empty_intersection_size()}." if (self.categorize_subsets().get('the empty intersection') and self.categorize_subsets().get('the empty intersection')!='largest_data_region') else "",
            "all_set_presence": f" An all set intersection is present with a size of {self.get_all_set_intersection_size()}." if self.get_all_set_intersection_size()!= None else f" An all set intersection is not present.",
            "intersection_trend_analysis":f"{self.calculate_intersection_trend()}",
            "individual_set_presence": f"{self.individual_set_presence()}",
            "low_set_presence": f"{self.low_set_presence()}",
            "high_set_presence": f"{self.high_set_presence()}",
            "medium_set_presence": f"{self.medium_set_presence()}",
            # Total number of elements in all sets, duplicates appear to be counted
            "universal_set_size": sum(self.data.sizes.values()),
            # Number of sets
            "set_count": len(self.data.sets),
            # Number of visible sets
            "visible_set_count": len(self.grammar.visible_sets),
            # List of set names
            "list_set_names": self.list_set_names,
            # List of visible set names
            "list_visible_set_names": self.list_visible_set_names,
            # Visual set sizes, sorted
            "sort_visible_sets": self.sort_visible_sets,
            # List of sorted visible set names and sizes
            "list_sorted_visible_sets": self.list_sorted_visible_sets,
            # Largest visible set name
            "max_set_name": self.truncate_string(self.sort_visible_sets()[0][0]),
            # Largest visible set size
            "max_set_size": self.sort_visible_sets()[0][1],
            # max set percentage
            "max_set_percentage": self.calculate_max_min_set_presence(
                self.sort_visible_sets()[0][0]
            ),
            # Smallest visible set name
            "min_set_name": self.truncate_string(self.sort_visible_sets()[-1][0]),
            # Smallest visible set size
            "min_set_size": self.sort_visible_sets()[-1][1],
            # min set percentage
            "min_set_percentage": self.calculate_max_min_set_presence(self.sort_visible_sets()[-1][0]),
            # Set Divergence
            "set_divergence": self.calculate_set_divergence,
            # largest intersection name and size
            "max_intersection_name": self.calculate_max_intersection()[0],
            "max_intersection_size": self.calculate_max_intersection()[1],
            # largest two intersections
            "largest_intersections": self.max_n_intersections(2),
            # largest two set or more intersections, conditional on largest_intersections not containing the same information
            "two_set_intersection": self.max_intersection_two_sets(),
            # other large intersections. conditional on largest_intersections not containing the same information
            "other_large_intersections": self.other_large_intersections(),
            # size of the largest set/intersection
            "min_size": min(self.data.count),
            # size of the smallest set/intersection
            "max_size": max(self.data.count),
            # Average size of all intersections
            "avg_size": self.avg_size,
            # Median size of all intersections
            "median_size": self.median_size,
            # 25th percentile for size
            "25perc_size": self.get_subset_percentile(SubsetField.SIZE, 25),
            # 75th percentile for size
            "75perc_size": self.get_subset_percentile(SubsetField.SIZE, 75),
            # Counts populated intersections
            "pop_intersect_count": len(self.data.subsets),
            # Counts non-empty visible intersections
            "non_empty_visible_intersect_count": self.count_non_empty_visible_subsets,
            # Counts non-empty intersections
            "non_empty_intersect_count": self.count_non_empty_subsets,
            # Number of visible non-empty intersections
            "visible_non_empty_intersect_count": self.count_non_empty_visible_subsets,
            # Number of total non-empty intersections
            "total_non_empty_intersect_count": self.count_non_empty_subsets,
            # a non terminal symbol, might move later
            "pop_non-empty_intersections": (
                f"There are {self.count_non_empty_subsets()} non-empty intersections, all of which are shown in the plot"
                if self.count_non_empty_subsets()
                == self.count_non_empty_visible_subsets()
                else f"There are {self.count_non_empty_subsets()} non-empty intersections, {self.count_non_empty_visible_subsets()} of which are shown in the plot"
            ),
            # Sort type for intersections
            "sort_type": self.grammar.sort_by.value,
            "sort_order": self.grammar.sort_order.value,
            # Number of intersections of each degree
            "list_degree_count": self.degree_count,
            # Number of intersections of each degree, their average size, and their average deviation
            "list_degree_info": self.degree_str(False),
            # Number of intersections of each degree, their average size,
            # their average deviation, and their total size
            "list_degree_info_verbose": self.degree_str(True),
            # Total subset size
            "subset_size": len(self.data.subsets),
            # 10 largest intersections by size- includes name, size, deviation
            "list_max_10int": self.max_n_intersections(10),
            # Largest 5 intersections by size, including name, size, deviation
            "list_max_5int": self.max_n_intersections(5),
            # List all intersections in order of size, including name, size, deviation
            "list_all_int": self.max_n_intersections(len(self.data.subsets)),
            "max_int_size": self.sort_subsets_by_key(SubsetField.SIZE, True)[0].size,
            "max_int_name": self.sort_subsets_by_key(SubsetField.SIZE, True)[0].name,
            "min_int_size": self.sort_subsets_by_key(SubsetField.SIZE, True)[-1].size,
            "min_int_name": self.sort_subsets_by_key(SubsetField.SIZE, True)[-1].name,
            # 90th percentile for size
            "90perc_size": self.get_subset_percentile(SubsetField.SIZE, 90),
            # 10th percentile for size
            "10perc_size": self.get_subset_percentile(SubsetField.SIZE, 10),
            # Total number of attributes
            "var_count": len(self.grammar.visible_atts),
            # List of attribute names
            "list_var_names": ", ".join(self.grammar.visible_atts),
            # Number of intersections with positive deviation
            "pos_dev_count": self.dev_info()["pos_count"],
            # Number of intersections with negative deviation
            "neg_dev_count": self.dev_info()["neg_count"],
            # Total size of positive deviation intersections
            "pos_dev_size": self.dev_info()["pos_size_total"],
            # Total size of negative deviation intersections
            "neg_dev_size": self.dev_info()["neg_size_total"],
            # Average positive deviation
            "avg_pos_dev": self.dev_info()["pos_avg"],
            # Average negative deviation
            "avg_neg_dev": self.dev_info()["neg_avg"],
            # Sizes of visible sets, listed
            "list_set_sizes": self.set_sizes,
            # 10 largest deviations, listed
            "list10_dev_outliers": self.dev_outliers(10) if len(self.data.subsets)>=10 else self.dev_outliers(len(self.data.subsets)),
            # 5 largest deviations, listed
            "list5_dev_outliers": self.dev_outliers(5) if len(self.data.subsets)>=5 else self.dev_outliers(len(self.data.subsets)),
            "category_of_subsets": self.categorize_subsets,
            "highest_dominant_set": self.find_dominant_sets(len(self.grammar.visible_sets)),
            "large_sets": self.find_sets_in_large_subsets(),
            "all_set_index": self.get_all_set_position(),
        }

    ###############################
    #       Public methods        #
    ###############################

    def get_token(self, token: str) -> str:
        """
        Return the string associated with the given token.
        If the token is unmapped, does not substitute it.
        Instead, returns it with curly braces around it.
        If the mapped value is not a string, float, int, or function,
        raises an exception.
        """
        if token not in self.map:
            # Substitute single curly braces so that the while loop doesn't go forever
            return "{" + token + "}"

        result: Any = self.map[token]
        if type(result) == float:
            return str(round(result, 2))
        elif type(result) == int:
            return str(result)
        elif type(result) == str:
            return result
        elif callable(result):
            return result()
        else:
            raise Exception("Invalid token type: " + str(type(result)))

    ###############################
    #           Helpers           #
    ###############################

    def sort_subsets_by_key(
        self, key: SubsetField, descending: bool = True
    ) -> "list[Subset]":
        """
        Returns the list of subsets from self.data.subsets,
        sorted by a specified key. The key must be a valid field
        in the dict or an error will be raised.
        Params:
          key: The key to sort by. Must be a valid field in the Subset class.
          descending: Whether to sort in descending order
        """
        return sorted(
            self.data.subsets, key=lambda x: getattr(x, key.value), reverse=descending
        )

    def degree_info(
        self, max_degree: int
    ) -> "Tuple[list[int], list[float], list[float], list[int]]":
        """
        Returns information about intersections of degrees up to max_degree.
        The information, in order, is:
            - The number of intersections of each degree
            - The average size of intersections of each degree
            - The average deviation of intersections of each degree

        This function only works if the data is not aggregated.
        Params:
            max_degree: The maximum degree to count to.
            Intersections with a degree greater than this are ignored.
            The returned lists will have length max_degree + 1. Since
            the lists are initialized with all 0s, all degree counts are accurate,
            but far more may be included than necessary.
        Returns:
            A tuple containing the four lists,
            where the list index corresponds to the degree.
            For the first list,
            the value at an index is the number of intersections with that degree.
            For the second list,
            the value at an index is the average size of intersections with that degree.
            For the third list,
            the value at an index is the average deviation of intersections with that degree.
            For the fourth list,
            the value at an index is the total size of intersections with that degree.
        """

        # 1 is added to each so that the max degree is included
        total_sizes: list[int] = [0] * (max_degree + 1)
        devs: list[float] = [0.0] * (max_degree + 1)
        degree_count: list[int] = [0] * (max_degree + 1)
        degree_count[0] = 1

        # Total all three values
        for subset in self.data.subsets:
            if subset.name == "Unincluded":
                total_sizes[0] += subset.size
                devs[0] += subset.dev

            degree = subset.degree
            if degree > max_degree:
                continue
            degree_count[degree] += 1
            total_sizes[degree] += subset.size
            devs[degree] += subset.dev

        # Convert totals to averages
        avg_sizes: list[float] = [0.0] * (max_degree + 1)
        for i in range(1, max_degree + 1):
            if degree_count[i] != 0:
                avg_sizes[i] = float(total_sizes[i]) / float(degree_count[i])
                devs[i] /= degree_count[i]
            # No need for else clause since the lists are initialized with 0s

        return degree_count, avg_sizes, devs, total_sizes

    def dev_info(self) -> dict[str, float]:
        """
        Returns a dictionary containing information about deviation.
        These overarching values are gathered only from non-empty intersections
        whose deviation is not 0.
        Dictionary keys:
            "pos_count": number of intersections with positive deviation
            "neg_count": number of intersections with negative deviation
            "pos_avg": average positive deviation
            "neg_avg": average negative deviation
            "pos_size_total": total size of positive deviations
            "neg_size_total": total size of negative deviations
            "pos_size_avg": average size of positive deviations
            "neg_size_avg": average size of negative deviations
        """
        pos_count: int = 0
        neg_count: int = 0
        pos_dev_total: int = 0
        neg_dev_total: int = 0
        pos_size_total: int = 0
        neg_size_total: int = 0

        for subset in self.data.subsets:
            if subset.dev > 0:
                pos_count += 1
                pos_dev_total += subset.dev
                pos_size_total += subset.size
            elif subset.dev < 0:
                neg_count += 1
                neg_dev_total += subset.dev
                neg_size_total += subset.size

        return {
            "pos_count": pos_count,
            "neg_count": neg_count,
            "pos_avg": pos_dev_total / pos_count if pos_count > 0 else 0,
            "neg_avg": neg_dev_total / neg_count if neg_count > 0 else 0,
            "pos_size_total": pos_size_total,
            "neg_size_total": neg_size_total,
            "pos_size_avg": pos_size_total / pos_count if pos_count > 0 else 0,
            "neg_size_avg": neg_size_total / neg_count if neg_count > 0 else 0,
        }

    ###############################
    #       Token functions       #
    ###############################

    def get_subset_percentile(self, field: SubsetField, perc: int) -> str:
        """
        Gets a percentile value for a specific field in this.data.subsets.
        Params:
          field: The field to get the percentile of.
          perc: The percentile to get. Must be between 0 and 100.
        """
        set_sort: list[Subset] = self.sort_subsets_by_key(field, False)
        index = int(len(set_sort) * perc / 100)
        return str(getattr(set_sort[index], field.value))

    def dev_outliers(self, n: int) -> str:
        """
        Returns a string listing the n largest intersections by absolute deviation,
        including the set name and its deviation
        """
        pos_sort: list[Subset] = self.sort_subsets_by_key(SubsetField.DEVIATION, True)
        neg_sort: list[Subset] = self.sort_subsets_by_key(SubsetField.DEVIATION, False)

        result: str = ""
        for i in range(0, n):
            if abs(pos_sort[0].dev) >= abs(neg_sort[0].dev):
                next_int: Subset = pos_sort.pop(0)
            else:
                next_int: Subset = neg_sort.pop(0)

            result += f"{next_int.name} ({next_int.dev}), "

        # Trim the trailing ', '
        return result[:-2]

    def set_sizes(self) -> str:
        """
        Returns string listing the size of each visible set.
        String is formatted as follows:
          "Set1: 10, Set2: 20, Set3: 30"
        """
        result: str = ""
        for setID, size in sorted(
            self.data.sizes.items(), key=lambda x: x[1], reverse=True
        ):
            # Trim "Set_" from the setID if extant to make it match up with the name field
            set_name: str = setID[4:] if setID.startswith("Set_") else setID
            if set_name in self.grammar.visible_sets:
                result += f"{set_name}: {size}, "

        # Trim the trailing ', '
        return result[:-2]

    def other_large_intersections(self) -> str:
        """
        Identifies and returns a string describing other large intersections involving large sets.

        This method first finds sets that are part of large subsets and then identifies the 
        largest intersections among them. If any of these large sets are found 
        within the already defined dominant sets, it returns an empty string
        Otherwise, it returns a formatted string indicating the involvement of other large intersections.

        Returns:
            str: A description of other large intersections involving large sets, or an empty 
            string if the largest intersections are found within the large sets.
        """
        large_sets = self.find_sets_in_large_subsets().strip()

        dominant_sets = self.find_dominant_sets(len(self.grammar.visible_sets))

        if large_sets in dominant_sets:
            return ""

        largest_intersections = [self.sort_subsets_by_key(SubsetField.SIZE, True)[i] for i in range(2)]

        # there are much better ways to implement this, but I am strapped for time...
        # TODO: improve this implementation
        # for every set in large_sets, check if it is entirely in any of the largest_intersections set membership list
        for intersection in largest_intersections:
            # initialize a flag to check if the set is in the intersection
            is_in_largest_intersections = True
            # remove 'and' and split the large_sets string to get individual set names
            for set in large_sets.replace('and', '').split(", "):
                # remove any whitespace
                set = set.strip()
                # if the set is not in the setmembership, we should break and set the flag to false
                # this is required because it is possible that there are multiple intersections that need to be checked
                if set not in intersection.setMembership:
                    is_in_largest_intersections = False
                    break
            if is_in_largest_intersections:
                return ""

        return f" Other large intersections also involve {large_sets}."

    def max_intersection_two_sets(self) -> str:
        """
        Determines the largest intersection of at least two sets and returns a descriptive string.

        This method calculates the maximum intersection of sets and constructs a string that describes
        the largest intersection in terms of its name and size. It also checks if the largest intersection
        is among the top two largest intersections.

        Returns:
            str: A string describing the largest intersection of at least two sets, including its name and size.
        """
        max_intersection = self.calculate_max_intersection()
        max_name = max_intersection[0]
        max_size = max_intersection[1]

        items = f"{self.grammar.metaData.items.lower()}" if self.grammar.metaData.items else "elements"

        largest_two_intersections = self.max_n_intersections(2)

        if max_name.replace("between", "") in largest_two_intersections:
            return ""

        return f" The largest intersection of at least two sets is {max_name}, with {max_size} {items}."

    def max_n_intersections(self, n: int) -> str:
        """
        Returns a string listing the n largest intersections
        by size, including their name, size, and deviation
        Params:
          n: Number of sets to list
        """
        sort: list[Subset] = self.sort_subsets_by_key(SubsetField.SIZE, True)
        result: str = ""
        for i in range(0, n):
            if i >= len(sort):
                break
            
            formatted_names = self.truncate_separately(sort[i].name)
            result += f"{formatted_names} ({sort[i].size}); "

        result = result.rstrip("; ")
        parts = result.split("; ")

        if len(parts) > 2:
            result = "; ".join(parts[:-1]) + "; and " + parts[-1]
        elif len(parts) == 2:
            result = " and ".join(parts)

        if len(sort) < n:
            return f"The largest {len(sort)} intersections are {result}"
        return f"The largest {n} intersections are {result}"

    def degree_count(self) -> str:
        """
        Returns a string describing the number of subsets with each degree.

        """
        result: str = ""

        for degree, count in enumerate(self.degree_info(20)[0]):
            if count == 0:
                continue
            result += f"{count} subsets with degree {degree}, "

        return result[:-2]  # Remove trailing comma and space

    def degree_str(self, verbose=False) -> str:
        """
        Returns a string describing the number of intersections of each degree,
        their average size, and if verbose, their average deviation and total size.
        Maximum degree listed is 20.

        Params:
            verbose: Whether to include average deviation and total size for each degree
        """
        result: str = ""
        counts, avg_sizes, devs, total_sizes = self.degree_info(20)

        # Start at 1 to skip the 0-degree/unincluded intersection
        for i in range(1, len(counts)):
            if counts[i] == 0:
                continue

            result += f"{counts[i]} subsets with degree {i} ({round(avg_sizes[i], 2)}"
            if verbose:
                result += f", {round(devs[i], 2)}, {total_sizes[i]}"
            result += "), "

        return result[:-2]

    def avg_size(self) -> str:
        """
        Returns the average size of all visible non-empty set intersections,
        rounded to an int.
        """
        count = 0
        total = 0
        for intersection in self.data.subsets:
            if intersection.size > 0:  # Check if the intersection is non-empty
                total += intersection.size
                count += 1

        if count > 0:
            average = total / count
        else:
            average = (
                0  # Avoid division by zero if there are no non-empty intersections
            )

        return str(int(average))

    def median_size(self) -> str:
        """
        Returns the median size of all set intersections,
        rounded to an int.
        """
        sort: list[Subset] = self.sort_subsets_by_key(SubsetField.SIZE, False)

        # Calculate the middle index
        mid = len(sort) // 2  # Divide and get floor

        # Check if the number of subsets is even
        if len(sort) % 2 == 0:  # Even number of elements
            median_val = (sort[mid - 1].size + sort[mid].size) / 2
        else:  # Odd number of elements
            median_val = sort[mid].size

        return str(int(median_val))

    def list_set_names(self) -> str:
        """
        Returns a string of the set names separated by commas,
        with the last two separated by "and".
        """
        return ", ".join(self.data.sets[:-1]) + " and " + self.data.sets[-1]

    def list_visible_set_names(self) -> str:
        """
        Returns a string of the visible set names separated by commas,
        with the last two separated by "and".
        """
        return (
            ", ".join(self.grammar.visible_sets[:-1])
            + " and "
            + self.grammar.visible_sets[-1]
        )

    def sort_visible_sets(self) -> dict[str, int]:
        """
        Returns a dictionary mapping visible set names to their sizes,
        sorted by size in descending order.
        """
        return sorted(
            self.grammar.visible_set_sizes.items(),
            key=lambda item: item[1],
            reverse=True,
        )

    def list_sorted_visible_sets(self) -> str:
        """
        Returns a string of the visible set names and sizes. The string should contain the 2nd largest set name with size,
        followed by the 3rd largest set name with size, and so on. The string should end with the smallest set name with size.
        Example string: "[SET2name] with [set2size], [SET3name] with [set3size],.. , and [SETnname] with [setnsize]"
        """
        # Sort the sets by size in descending order and exclude the largest set
        sorted_by_size = sorted(
            self.grammar.visible_set_sizes.items(),
            key=lambda item: item[1],
            reverse=True,
        )[1:]

        # Format the sorted sets into the desired string format
        if len(sorted_by_size) > 1:
            set_strings = [
                f"{self.truncate_string(set_name)} with {size}" for set_name, size in sorted_by_size[:-1]
            ]
            last_set_string = f"{self.truncate_string(sorted_by_size[-1][0])} with {sorted_by_size[-1][1]}"
            return ", ".join(set_strings) + ", and " + last_set_string
        elif sorted_by_size:
            # If there is only one set after excluding the largest
            return f"{self.truncate_string(sorted_by_size[0][0])} with {sorted_by_size[0][1]}"
        else:
            # If there are no sets to list (empty or only one set was visible initially)
            return "No sets to list"

    def count_non_empty_visible_subsets(self) -> int:
        """
        Counts the number of subsets with a size greater than zero.

        Returns:
            int: The count of non-empty subsets.
        """
        non_empty_count = 0

        # Iterate through each subset in the list
        for subset in self.data.subsets:
            # Check if the size of the subset is greater than zero
            if subset.size > 0:
                non_empty_count += 1

        return non_empty_count

    def count_non_empty_subsets(self) -> int:
        """
        Counts the number of subsets with a size greater than zero.

        Returns:
            int: The count of non-empty subsets.
        """
        non_empty_count = 0

        # Iterate through each subset in the list
        for subset in self.data.all_subsets:
            # Check if the size of the subset is greater than zero
            if subset.size > 0:
                non_empty_count += 1

        return non_empty_count

    def calculate_max_min_set_presence(self, maxmin_sized_set_name) -> str:
        """
        Calculate the percentage of non-empty intersections where the largest and smallest sets are present.
        """
        # Counters for the number of non-empty intersections including the largest and smallest sets
        maxmin_set_count = 0

        # Total number of non-empty intersections
        total_non_empty = sum(1 for subset in self.data.subsets if subset.size > 0)

        # Iterate through all subsets
        for subset in self.data.subsets:
            if subset.size > 0:  # Check only non-empty subsets
                if maxmin_sized_set_name in subset.name:
                    maxmin_set_count += 1

        # Calculate percentages
        maxmin_set_percentage = (
            (maxmin_set_count / total_non_empty) * 100 if total_non_empty else 0
        )

        return f"{maxmin_set_percentage:.1f}%"

    def calculate_max_intersection(self) -> dict[str, int]:
        """
        Calculate the largest intersection size and name that contains more than one set.
        """
        largest_size = 0
        largest_subset = None

        for subset in self.data.subsets:
            if subset.degree > 1 and subset.size > largest_size:
                largest_size = subset.size
                largest_subset = subset

        # 'largest_subset' now holds the subset with more than one set that has the largest size
        if largest_subset is not None:
            truncated_names = []
            sorted_subset = largest_subset.name
    
            for name in sorted_subset.split(', '):
                if name == "the empty intersection":
                    truncated_names.append(name)
                else:
                    truncated_names.append(self.truncate_string(name))

        
            if len(truncated_names) == 2:
                formatted_names = "between " + " and ".join(truncated_names)
            elif len(truncated_names) > 2:
                formatted_names = "between " + ", ".join(truncated_names[:-1]) + ", and " + truncated_names[-1]
            else:
                formatted_names = "Just " + truncated_names[0]
                formatted_names = truncated_names[0] if truncated_names[0] == "the empty intersection" else "Just " + truncated_names[0]

            return formatted_names, largest_subset.size
            
        else:
            return None, 0

        
    def calculate_min_intersection(self) -> dict[str, int]:
        """
        Calculate the smallest intersection size and name that contains more than one set.
        """
        smallest_size = float('inf')
        smallest_subset = None

        for subset in self.data.subsets:
            if subset.degree > 1 and subset.size < smallest_size:
                smallest_size = subset.size
                smallest_subset = subset

        # 'smallest_subset' now holds the subset with more than one set that has the smallest size
        if smallest_subset is not None:
            return self.truncate_separately(smallest_subset.name), smallest_subset.size
        
        else:
            return None, 0
        
   
    def calculate_set_divergence(self):
        max_set_size = self.sort_visible_sets()[0][1]
        min_set_size = self.sort_visible_sets()[-1][1]
        
        divergence_percentage = (min_set_size / max_set_size) * 100
        divergence_percentage = math.ceil(divergence_percentage)

        # Determine the divergence category
        if divergence_percentage < 30:
            return IndividualSetSize.DIVERGINGALOT.value
        elif 30 <= divergence_percentage <= 90:
            return IndividualSetSize.DIVERGING.value
        elif divergence_percentage > 90:
            return IndividualSetSize.DIVERGINGABIT.value


    def calculate_change_trend(self):
        """
        Analyzes the trend of changes in intersection sizes and classifies the trend.

        This method calculates the trend of changes in intersection sizes using three types of fits:
        linear, exponential, and quadratic polynomial. It then compares the residuals of these fits to determine
        the best fitting model and classifies the trend based on the parameters of the best fit.

        Returns:
            IntersectionTrend: An enumeration value representing the classified trend:
                - IntersectionTrend.DRASTIC: If the exponential fit is the best and the decay rate (beta) is greater than 0.8.
                - IntersectionTrend.RAPID: If the exponential fit is the best but the decay rate (beta) is less than or equal to 0.8.
                - IntersectionTrend.QUICK: If the quadratic polynomial fit is the best.
                - IntersectionTrend.STEADY: If the linear fit is the best.
        """
        intersection_sizes = [self.data.subsets[i].size for i in range(len(self.data.subsets))]

        x = np.arange(len(intersection_sizes))
        y = np.array(intersection_sizes)

        # linear fit
        slope, intercept, r_value, p_value, _ = stats.linregress(x, y)
        linear_fit = slope * x + intercept
        linear_residuals = np.sum((y - linear_fit) ** 2)

        # polynomial (quadratic) fit
        fit = np.polyfit(x, y, 2, full=True)
        quadratic_residuals = fit[1][0]

        # exponential fit
        popt, _ = curve_fit(lambda x, a, beta, c: a*np.exp(-beta*x)+c, x, y,
                            p0=[max(y), 0.1, min(y)],
                            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]),
                            maxfev=5000)
        a, beta, c = popt

        if beta > 0 and a > 0:
            x_fit = np.linspace(0, len(x)-1, 100)
            y_fit = a * np.exp(-beta * x_fit) + c
            y_fit_interpolated = np.interp(x, x_fit, y_fit)  # Interpolate y_fit to match x
            exponential_residuals = np.sum((y - y_fit_interpolated) ** 2)
        else:
            exponential_residuals = np.inf

        if exponential_residuals < linear_residuals and exponential_residuals < quadratic_residuals:
            if beta > 0.8:
                return IntersectionTrend.DRASTIC.value
            else:
                return IntersectionTrend.RAPID.value
        elif quadratic_residuals < linear_residuals and quadratic_residuals < exponential_residuals:
            return IntersectionTrend.QUICK.value
        else:
            return IntersectionTrend.STEADY.value

    def calculate_largest_factor(self):
        sorted_sizes = self.sort_subsets_by_key(SubsetField.SIZE, True)
        if len(sorted_sizes) >= 2:
            largest_size = sorted_sizes[0].size
            second_largest_size = sorted_sizes[1].size
            
            if second_largest_size > 0:
                factor = largest_size / second_largest_size
                decimal_part = factor - math.floor(factor)
                # Apply ceiling for > 0.8, otherwise floor
                if decimal_part > 0.78:
                    adjusted_factor = math.ceil(factor)
                else:
                    adjusted_factor = math.floor(factor)
                return int(adjusted_factor)
        return None  


    def categorize_subsets(self):
        """
        Categorize the subsets into small, medium, large and largest regions based on their coeficent of variance.
        Based on the varainces, we do set two thresholds for large region of intersections and small region intersections
        Then we assign the intersection types (individual, low-degree, medium-degree, high-order sets)  based on the percentage presence (>=50%)
        Returns a dictionary of intersection types being on the region(s)

        """
        results = {}
        sorted_subsets = sorted(self.data.subsets, key=lambda subset: subset.size, reverse=True)
        
        largest_subset = sorted_subsets.pop(0)  
        
        region_classification = RegionClassification()
        region_classification.set_largest(largest_subset)

        if not sorted_subsets:
            return {'largest_data_region': [largest_subset]}

        sizes = [subset.size for subset in sorted_subsets]
        unique_sizes = sorted(set(sizes), reverse=True)  

        q1, median, q3 = np.percentile(sizes, [25, 50, 75])
        iqr = q3 - q1
        multiplier = 1.5

        large_threshold = median + multiplier * iqr

        size_categories = {}

        for size in unique_sizes:
            if size > large_threshold:
                size_categories[size] = 'large'
            elif size < median:
                size_categories[size] = 'small'
            else:
                size_categories[size] = 'medium'


        for subset in sorted_subsets:
            category = size_categories[subset.size]
            if category == 'large':
                region_classification.add_to_large_region(subset)
            elif category == 'medium':
                region_classification.add_to_medium_region(subset)
            else: 
                region_classification.add_to_small_region(subset)


        # the small treshold may not always perform well, specially in the case of being the smallest intersection really pretty smaller than the largest
        if not region_classification.large_data_region and region_classification.medium_data_region:
            largest_medium = max(region_classification.medium_data_region, key=lambda x: x.size)
            region_classification.medium_data_region = [x for x in region_classification.medium_data_region if x.size != largest_medium.size]
            region_classification.large_data_region = [x for x in sorted_subsets if x.size == largest_medium.size]
        
        if not region_classification.small_data_region and region_classification.medium_data_region:
            smallest_medium = min(region_classification.medium_data_region, key=lambda x: x.size)
            region_classification.medium_data_region = [x for x in region_classification.medium_data_region if x.size != smallest_medium.size]
            region_classification.small_data_region = [x for x in sorted_subsets if x.size == smallest_medium.size]

        regions = {
            'largest_data_region': [region_classification.largest_data_region],
            'large_data_region': region_classification.large_data_region,
            'medium_data_region': region_classification.medium_data_region,
            'small_data_region': region_classification.small_data_region, 
        }
       
        regions = {
        'largest_data_region': [region_classification.largest_data_region],
        'large_data_region': region_classification.large_data_region,
        'medium_data_region': region_classification.medium_data_region,
        'small_data_region': region_classification.small_data_region, 
        }

        total_sizes = {region: sum(subset.size for subset in subsets) for region, subsets in regions.items()}

        for region_name, subsets in regions.items():
            special_sizes = {}
            classification_sizes = Counter()

            for subset in subsets:
                if subset.classification in ['the empty set']:
                    special_sizes[subset.classification] = subset.size
                else:
                    classification_sizes[subset.classification] += subset.size
            
            percentages = {cls: (size / total_sizes[region_name] * 100) for cls, size in classification_sizes.items()}

            results[region_name] = {**percentages, **special_sizes}

        classification_to_regions = {}
        
        threshold_percentage = 50.0  # the percentage threshold for inclusion

        for region, classifications in results.items():
            above_threshold = {classification: value for classification, value in classifications.items() if value >= threshold_percentage}
            
            if above_threshold:
                for classification in above_threshold.keys():
                    classification_to_regions.setdefault(classification, set()).add(region)
            else:
                if classifications:
                    max_value = max(classifications.values())
                    max_classifications = [classification for classification, value in classifications.items() if value == max_value]
                    for classification in max_classifications:
                        classification_to_regions.setdefault(classification, set()).add(region)



        for special_case in ['the empty set', 'all set']:
            if special_case in results:
                for region in results[special_case]:
                    classification_to_regions[special_case] = {region}


        final_output = {cls.value: {regions} if isinstance(regions, str) else set(regions) for cls, regions in classification_to_regions.items()}
        
        return final_output

    def get_empty_intersection_size(self):
    # Iterate through subsets to find 'the empty intersection'
        for subset in self.data.subsets:
            if subset.classification.value == 'the empty intersection':
                return subset.size
        # Return 0 or None if 'the empty intersection' is not found
        return None

    def get_all_set_intersection_size(self):
        for subset in self.data.subsets:
            if subset.degree == self.data.all_sets_length:
                return subset.size
        # Return 0 or None if 'all set' intersection is not found
        return None
    
    def individual_set_presence(self) -> str:
        categorization = self.categorize_subsets()
        individual_set_regions = categorization.get('individual set')

        if individual_set_regions:
            regions_list = list(individual_set_regions)
            if len(regions_list) == 1:
                return f" The individual set intersections are seen in {regions_list[0].replace('_data_region', '')} sized intersections."
            else:
                regions_formatted = [region.replace('_data_region', '') for region in regions_list]
                return f" The individual set intersections are significantly present in {' and '.join(regions_formatted)} intersections."
        else:
            return ""
        
    def medium_set_presence(self) -> str:
        categorization = self.categorize_subsets()
        medium_set_regions = categorization.get('medium set')

        if medium_set_regions:
            regions_list = list(medium_set_regions)
            if len(regions_list) == 1:
                return f" The medium degree set intersections can be seen among {regions_list[0].replace('_data_region', '')} sized intersections."
            else:
                regions_formatted = [region.replace('_data_region', '') for region in regions_list]
                return f" The medium degree set intersections can be seen among {' and '.join(regions_formatted)} sized intersections."
        else:
            return ""
        
    def low_set_presence(self) -> str:
        categorization = self.categorize_subsets()
        low_set_regions = categorization.get('low set')

        if low_set_regions:
            # Convert set to list to index
            regions_list = list(low_set_regions)
            if len(regions_list) == 1:
                return f" The low degree set intersections lie in {regions_list[0].replace('_data_region', '')} sized intersections."
            else:
                regions_formatted = [region.replace('_data_region', '') for region in regions_list]
                return f" The low degree set intersections lie in {' and '.join(regions_formatted)} sized intersections."
        else:
            return ""
        
    def high_set_presence(self) -> str:
        categorization = self.categorize_subsets()

        high_set_regions = categorization.get('high order set')

        if high_set_regions:
            regions_list = list(high_set_regions)
            if len(regions_list) == 1:
                if regions_list[0] == 'largest_data_region':
                    return " The high order set intersections are the largest."
                return f" Among the {regions_list[0].replace('_data_region', '')} sized intersections, the high order set intersections are significantly present."
            else:
                regions_formatted = [region.replace('_data_region', '') for region in regions_list]
                return f" In {' and '.join(regions_formatted)} sized intersections, the high order set intersections are significantly present."
        else:
            return ""
       

    def calculate_intersection_trend(self) -> str:
        intersection_trend = self.calculate_change_trend()

        max_int_size = self.sort_subsets_by_key(SubsetField.SIZE, True)[0].size
        min_int_size = self.sort_subsets_by_key(SubsetField.SIZE, True)[-1].size

        return f" The intersection sizes peak at a value of {max_int_size} and then {intersection_trend} flatten down to {min_int_size}."

    def find_dominant_intersections(self):
        """
        Identify and return the dominant intersections from the dataset.

        This method calculates the average size of intersections and filters out
        the intersections that have a size greater than this average.

        Returns:
            list: A list of subsets where each subset has a size greater than the average size.
        """
        # use the average size intersection and get all intersections that are of greater size than this
        avg_size = self.avg_size()

        dominant_intersections = [subset for subset in self.data.subsets if float(subset.size) > float(avg_size)]

        return dominant_intersections

    def find_dominant_sets(self, visible_sets):
        """
        Identifies and returns a description of the dominant sets based on their occurrences in dominant intersections.

        Args:
            visible_sets (int): The number of most common sets to consider.

        Returns:
            str: A description of the dominant sets, indicating which sets are dominant and their occurrence percentages.

        The method works as follows:
        1. It calculates the occurrences of each set in the dominant intersections.
        2. It filters the sets based on an 80% occurrence threshold.
        3. It generates a descriptive string indicating the dominant sets and their occurrences.
        """
        set_occurrences = Counter()

        dominant_intersections = self.find_dominant_intersections()

        # get common occurences for each set
        for subset in dominant_intersections:
            for set_name in self.grammar.visible_sets:
                if set_name in subset.name:
                    set_occurrences[set_name] += 1

        most_common_sets = set_occurrences.most_common(visible_sets)

        # 80% threshold for "dominant" set
        THRESHOLD = 80

        filtered_sets = []

        # for each value in most_common_sets, filter by the percentage threshold
        for set_name, count in most_common_sets:
            percentage = (count / len(dominant_intersections)) * 100
            if percentage >= THRESHOLD:
                filtered_sets.append((set_name, count, percentage))

        result = ""

        if len(filtered_sets) == 1:
            if filtered_sets[0][2] == 100:
                result = f" {self.truncate_string(filtered_sets[0][0])} is the dominant set, appearing in all major intersections."
            else:
                result = f" {self.truncate_string(filtered_sets[0][0])} is the dominant set with {filtered_sets[0][1]} occurrences."
        elif len(filtered_sets) > 1:
            result = f" {self.truncate_separately(', '.join([set_name[0] for set_name in filtered_sets]))} are the dominant sets."

        return result

    def find_sets_in_large_subsets(self):
        sorted_subsets = sorted(self.data.subsets, key=lambda subset: subset.size, reverse=True)

        # Check the top two largest subsets and remove them if they have empty setMembership. Remove the second and third largest, if empty
        if len(sorted_subsets) > 1 and len(sorted_subsets[1].setMembership) == 0:
            sorted_subsets.pop(1)
        elif len(sorted_subsets) > 2 and len(sorted_subsets[2].setMembership) == 0:
            sorted_subsets.pop(2)

        # Extract set names from the 2nd largest subset
        second_largest_sets = sorted_subsets[1].setMembership
        sets = []

        if len(second_largest_sets) == 1:
            # Extract set names from the 3rd largest subset
            third_largest_sets = sorted_subsets[2].setMembership
            # Find the intersection of sets between the 2nd and 3rd largest subsets
            common_sets = second_largest_sets.union(third_largest_sets)

            for cs in common_sets:
                sets.append(cs)

        else:
            for sm in second_largest_sets:
                sets.append(sm)

        if len(sets) == 2:
            return f"{self.truncate_string(sets[0])} and {self.truncate_string(sets[1])}"
        elif len(sets) > 2:
            return self.truncate_separately(', '.join(sets))
        else:
            return self.truncate_string(sets)

    def get_all_set_position(self):
        sorted_subsets = sorted(self.data.subsets, key=lambda subset: subset.size, reverse=True)

        # Find the "all set" intersection if it exists
        all_set_index = None
        for index, subset in enumerate(sorted_subsets):
            if subset.degree == len(self.grammar.visible_sets):  # all_sets_length is equal to the number of visible sets
                all_set_size = subset.size
                all_set_index = index
                break

        # Determine the position of the "all set" intersection
        if all_set_index is not None:
            total_subsets = len(sorted_subsets)
            if all_set_index == 0:
                return f"The intersection of all sets is the largest with {all_set_size} elements."
            elif all_set_index == 1:
                return f"The intersection of all sets is the second largest with {all_set_size} elements."
            elif all_set_index == 2:
                return f"The intersection of all sets is the third largest with {all_set_size} elements."
            elif all_set_index == total_subsets - 1:
                return f"The intersection of all sets is the smallest with {all_set_size} elements."
            elif all_set_index == total_subsets - 2:
                return f"The intersection of all sets is the second smallest with {all_set_size} elements."
            elif all_set_index == total_subsets - 3:
                return f"The intersection of all sets is the third smallest with {all_set_size} elements."
            else:
                return f"The intersection of all sets is present with {all_set_size} elements."
        else:
            return ""


    def truncate_string(self, original_string):
        if original_string.lower().startswith('just '):
            original_string = original_string[5:]
        if original_string.lower().startswith('and '):
            original_string = original_string[4:]

        if TokenMap.TRUNCATION_LENGTH < len(original_string):
            return original_string[:TokenMap.TRUNCATION_LENGTH]
        return original_string
    
    def truncate_separately(self, sorted_subset):
        
        """
        Splits a string containing multiple set names separated by commas,
        truncates each name to a maximum length defined by TokenMap.TRUNCATION_LENGTH,
        and formats them into a single string.

        Initially for convenience to truncate, we cut down 'and' and 'Just' from the subset name in the truncate_string function.
        In this function, if the subset contains multiple set names, they are joined into a formatted string
        with commas separating all but the last two names, which are separated by ", and".
        If the subset contains only one set name, it prefixes the name with "Just".

        Parameters:
        sorted_subset (str): A comma-separated string of set names.

        Returns:
        str: A string of formatted and truncated set names.
        """
        truncated_names = []
    
        for name in sorted_subset.split(', '):
            if name == "the empty intersection":
                truncated_names.append(name)
            else:
                truncated_names.append(self.truncate_string(name))

    
        if len(truncated_names) == 2:
            formatted_names = " and ".join(truncated_names)
        elif len(truncated_names) > 2:
            formatted_names = ", ".join(truncated_names[:-1]) + ", and " + truncated_names[-1]
        else:
            formatted_names = truncated_names[0] if truncated_names[0] == "the empty intersection" else "just " + truncated_names[0]

        return formatted_names
